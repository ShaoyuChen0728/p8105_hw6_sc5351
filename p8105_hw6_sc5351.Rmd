---
title: "p8105_hw6_sc5351"
author: "Shaoyu Chen"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(viridis)
library(dplyr)
set.seed(1)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	fig.width = 8, 
  fig.height = 6,
  out.width = "90%"
)

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

theme_set(theme_minimal() + theme(legend.position = "bottom"))
```

```{r load_libraries}
library(tidyverse)
library(modelr)
```

### Problem 1

#Create a city_state variable (e.g. "Baltimore, MD"), and a binary variable indicating whether the homicide is solved. Omit cities Dallas, TX; Phoenix, AZ; and Kansas City, MO -- these don't report victim race. Also omit Tulsa, AL -- this is a data entry mistake. For this problem, limit your analysis those for whom victim_race is white or black. Be sure that victim_age is numeric.

```{r q1_data_cleaning}
homicide_df = 
  read_csv("./data/homicide-data.csv", na = c("", "NA", "Unknown")) |> 
  mutate(
    city_state = str_c(city, state, sep = ", "),
    victim_age = as.numeric(victim_age),
    resolution = case_when(
      disposition == "Closed without arrest" ~ 0,
      disposition == "Open/No arrest"        ~ 0,
      disposition == "Closed by arrest"      ~ 1)
  ) |> 
  filter(victim_race %in% c("White", "Black")) |> 
  filter(!(city_state %in% c("Tulsa, AL", "Dallas, TX", "Phoenix, AZ", "Kansas City, MO"))) |> 
  select(city_state, resolution, victim_age, victim_sex, victim_race)
```

#For the city of Baltimore, MD, use the glm function to fit a logistic regression with resolved vs unresolved as the outcome and victim age, sex and race as predictors. Save the output of glm as an R object; apply the broom::tidy to this object; and obtain the estimate and confidence interval of the adjusted odds ratio for solving homicides comparing male victims to female victims keeping all other variables fixed.

```{r q1_glm_baltimore}
baltimore_glm = 
  filter(homicide_df, city_state == "Baltimore, MD") |> 
  glm(resolution ~ victim_age + victim_sex + victim_race, family = binomial(), data = _)

baltimore_glm |> 
  broom::tidy() |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(OR, OR_CI_lower, OR_CI_upper) |>
  knitr::kable(digits = 3)
```

#Now run glm for each of the cities in your dataset, and extract the adjusted odds ratio (and CI) for solving homicides comparing male victims to female victims. Do this within a "tidy" pipeline, making use of purrr::map, list columns, and unnest as necessary to create a dataframe with estimated ORs and CIs for each city.

```{r q1_glm_all_cities}
model_results = 
  homicide_df |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolution ~ victim_age + victim_sex + victim_race, 
                             family = binomial(), data = df)),
    tidy_models = map(models, broom::tidy)) |> 
  select(-models, -data) |> 
  unnest(cols = tidy_models) |> 
  mutate(
    OR = exp(estimate), 
    OR_CI_upper = exp(estimate + 1.96 * std.error),
    OR_CI_lower = exp(estimate - 1.96 * std.error)) |> 
  filter(term == "victim_sexMale") |> 
  select(city_state, OR, OR_CI_lower, OR_CI_upper)

model_results |>
  slice(1:5) |> 
  knitr::kable(digits = 3)
```

#Create a plot that shows the estimated ORs and CIs for each city. Organize cities according to estimated OR, and comment on the plot.

```{r q1_plot}
model_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = OR_CI_lower, ymax = OR_CI_upper)) + 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

### Problem 2

#First,download the weather data.

```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2022-01-01",
    date_max = "2022-12-31") |>
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) |>
  select(name, id, everything())
```

#Create bootstrap function 
```{r}
boot_sample = function(df) {
  
  sample_frac(df, replace = TRUE)
  
}
```

#Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of rsquare
```{r}
boot_r_data = 
  weather_df |> 
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap,  ~lm(tmax ~ tmin + prcp, data = .)),
    glance = map(models, broom::glance))|>
  select(-strap,-models)|>
  unnest(glance)
boot_r_data
```

#Plot the distribution of rsquare
```{r}
r_plot =
  boot_r_data|>
  ggplot(aes(x = r.squared)) +
  geom_density()
r_plot
```
The plot is a left-skewed of R-squared, and the peak of the density is around 0.92.

#construct 95% confidence interval of rsquare.
```{r}
r_squared_ci = 
  boot_r_data |>
  select(r.squared) |>
  summarize(
    ci_lower = quantile(r.squared, 0.025),
    ci_upper = quantile(r.squared, 0.975)
  )

r_squared_ci
```
A 95% confidence interval for the rsquared is between 0.889 and 0.941. 

#Use 5000 bootstrap samples and, for each bootstrap sample, produce estimates of log(beta1*beta2).
```{r}
boot_log_data = 
  weather_df |> 
  modelr::bootstrap(n = 5000) |>
  mutate(
    models = map(strap, ~lm(tmax ~ tmin + prcp, data = .)),
    results = map(models, broom::tidy))|>
  select(-strap,-models)|>
  unnest(results) |>
  select(.id, term, estimate) |> 
  pivot_wider(
    names_from = term, 
    values_from = estimate) |> 
  rename(beta1 = tmin, beta2 = prcp) |>
  mutate(log_b1_b2 = log(beta1 * beta2))
boot_log_data
```
```{r}
log_beta_plot =
  boot_log_data|>
  filter(log_b1_b2 != "NaN") |>
  select(log_b1_b2)|>
  ggplot(aes(x = log_b1_b2)) +
  geom_density()
log_beta_plot
```
The plot is a left-skewed of R-squared, and the peak of the density is around -5.5.

#construct 95% confidence interval of log(beta1*beta2).
```{r}
log_beta_ci =
  boot_log_data|>
  select(log_b1_b2)|>
  summarize(
    ci_lower = quantile(log_b1_b2, 0.025,na.rm = TRUE),
    ci_upper = quantile(log_b1_b2, 0.975,na.rm = TRUE)
  )
log_beta_ci
```
A 95% confidence interval for the log(beta1 * beta2) is between -8.896 and -4.590.


### Problem 3

#data clean
```{r}
birthweight_data =
  read_csv("./data/birthweight.csv")|>
  janitor::clean_names()|>
  mutate(babysex = factor(case_when(
                           babysex == 1 ~ "male",
                           babysex == 2 ~ "female")),
         
         frace = factor(case_when(
                           frace == 1 ~ "White",
                           frace == 2 ~ "Black",
                           frace == 3 ~ "Asian",
                           frace == 4 ~ "Puerto Rican",
                           frace == 8 ~ "Other",
                           frace == 9 ~ "Unknown")),
         
         malform = factor(case_when(
                           malform == 0 ~ "absent",
                           malform == 1 ~ "present")),
         
         mrace = factor(case_when(mrace == 1 ~ "White",
                           mrace == 2 ~ "Black",
                           mrace == 3 ~ "Asian",
                           mrace == 4 ~ "Puerto Rican",
                           mrace == 8 ~ "Other")))
```
#then, check the missing data.
```{r}
sum(is.na(birthweight_data))
```
There is no missing data in birthweight_data.

#Regression Models for birthweight_data.
```{r}
bwt_model = lm(bwt ~ gaweeks + ppwt + momage + smoken + wtgain + ppbmi, data = birthweight_data)
summary(bwt_model)
```
Based on the results, I find that `gaweeks`,`ppwt`,`momage`,`wtgain`,and `ppbmi`are significant variables in babyâ€™s birth weight.

#Create a plot of model residuals against fitted values.
```{r}
birthweight_data |>
  add_predictions(bwt_model)|>
  add_residuals(bwt_model)|>
  ggplot(aes(x = pred, y = resid))+
  geom_point()+
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values",
    y = "Residuals"
  ) +
  stat_smooth(method = "lm")
```
#Compare your model to two others
```{r}
cv_data =
  crossv_mc(birthweight_data, 100) |> 
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble)) |> 
  mutate(
    my_mod = map(.x = train, ~lm(bwt ~ gaweeks + ppwt + momage + smoken + wtgain + ppbmi, data = birthweight_data)),
    main_mod = map(.x = train, ~lm(bwt ~ blength + gaweeks, data = birthweight_data)),
    interaction_mod = map(.x = train, ~lm(bwt ~ bhead + blength + babysex + babysex*bhead*blength, data = birthweight_data)))|> 
  mutate(
    rmse_my_mod = map2_dbl(my_mod, test, ~rmse(model = .x, data = .y)),
    rmse_main_mod = map2_dbl(main_mod, test, ~rmse(model = .x, data = .y)),
    rmse_interaction_mod = map2_dbl(interaction_mod, test, ~rmse(model = .x, data = .y)))

    
```

#Create the plot
```{r}
cv_data |>
  select(starts_with("rmse"))|>
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_")|>
  mutate(model = fct_inorder(model))|>
  ggplot(aes(x = model, y = rmse, fill = model)) + 
  geom_violin()
```

By comparing the three models, the intersection model has the smallest RMSE distribution. Lower RMSE values represents better, so the intersection model performs best.